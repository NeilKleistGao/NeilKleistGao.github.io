<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="I thought what I'd do was I'd pretend I was one of those deaf-mutes"><meta name="theme-color" content="#2d4356"><meta name="baidu-site-verification"><title>Conjugate Gradient | NeilKleistGao's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="NeilKleistGao's Blog" type="application/atom+xml">
</head><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><body class="night"><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">LITREILY</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">NeilKleistGao's Blog</a></h1></div><p class="m-desc">I thought what I'd do was I'd pretend I was one of those deaf-mutes.</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">Archives</a></li><li><span class="dot">●</span><a href="/categories/">Categories</a></li><li><span class="dot">●</span><a href="/about/">About</a></li><li class="m-sch"><form class="form" id="j-formsch" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="search" onfocus="if(this.value=='search'){this.value='';}" onblur="if(this.value==''){this.value='search';}"><input type="text" style="display:none;"></form></li></ul><div id="local-search-result"></div></div></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">Conjugate Gradient</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/2020/10/08/conjugate-gradient/">2020-10-08</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/Notes/">Notes</a></span></div></div><div class="p-content"><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本篇文章是关于课程“数值分析与计算”中共轭梯度方法部分的笔记，包含该算法的部分推导和算法的实现，而忽略算法的正确性与收敛性证明和误差分析。</p>
<p>为了节约时间，本文使用中文编写。</p>
<a id="more"></a>
<h1 id="向量求导"><a href="#向量求导" class="headerlink" title="向量求导"></a>向量求导</h1><p>向量求导有相当多的形式（如：向量对标量求导，标量对向量求导，向量对向量求导）。为了节省篇幅，我们只说明对本文最重要的一个：标量对向量的求导。</p>
<p>对于一个函数<script type="math/tex">y = f(x)</script>，其中<script type="math/tex">x = [x_1, x_2, \dots, x_n]^T</script>为向量，<script type="math/tex">y</script>为标量，梯度<script type="math/tex">\nabla f = \frac{\partial y}{\partial x} = [\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \dots, \frac{\partial y}{\partial x_n}]</script>。其实本质就是把输入的向量看成<script type="math/tex">n</script>个变量，把<script type="math/tex">f(x)</script>看作一个多元函数，对其求梯度。</p>
<h1 id="最速下降法"><a href="#最速下降法" class="headerlink" title="最速下降法"></a>最速下降法</h1><h2 id="问题转换"><a href="#问题转换" class="headerlink" title="问题转换"></a>问题转换</h2><p>我们希望求解形如<script type="math/tex">Ax = b</script>的线性方程组。对于一般形式的矩阵，我们需要使用高斯消元；而对于特殊形式的矩阵，如对称正定矩阵，我们可以使用迭代的方式更方便地进行求解。</p>
<p>然而对于<script type="math/tex">Ax = b</script>，我们很难直接进行迭代求解（事实上，对于某些满足特殊条件的矩阵，我们可以进行一些简单的变换从而进行迭代，详见Jacobi算法和Gauss-Seidel算法）。迭代法本身更擅长于解决优化问题而不是线性方程组，所以我们试着把问题转移到最小化问题上。</p>
<p>将视线拉回到最前面提到的二次型上：<script type="math/tex">f(x) = \frac{1}{2}x^TAx - bx + c</script>。这个函数拥有一个全局最小值，并且可以使用迭代进行求解；最重要的是：这个函数的梯度<script type="math/tex">\nabla f = Ax - b</script>！而且当梯度为0时，刚好可以到达函数的最小值。</p>
<p>为了求出具体的数值，我们采取线性搜索的策略：每次选定一个方向，并在这个方向上进行搜索，找到该方向上的最小值，再根据更新后的位置，重新选取方向迭代。重复上述步骤，直到梯度被优化为0。</p>
<p>我们使用<script type="math/tex">r = b - Ax</script>来表示每次残差，即离真实值的距离；并令<script type="math/tex">p</script>表示我们每次迭代的方向向量。对于当前的迭代步骤<script type="math/tex">x_i</script>，我们有<script type="math/tex">f(x_{i + 1}) = f(x_i + \alpha_ip_i)</script>，当且仅当导函数为0时，我们找到了该方向的最小值点。所以</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial \alpha}f(x_{i + 1}) = f'(x_{x + 1})^Tp_i = (Ax_{i + 1} - b)^Tp_i</script><p>根据梯度的性质，我们知道：随梯度负方向下降的速度是最快的。所以我们让<script type="math/tex">p_i</script>为当前点的梯度方向，有：</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial \alpha}f(x_{i + 1}) = -r_{i + 1}^Tr_i = 0</script><p>我们将<script type="math/tex">\alpha_i</script>求解出来：<script type="math/tex">r_{i + 1}^Tr_i = (b - A(x_i + \alpha_ir_i))^Tr_i = 0</script>，解得：<script type="math/tex">\alpha_i = \frac{r_i^Tr_i}{r_i^TAr_i}</script>。这样我们就可以进行迭代了。</p>
<h2 id="为何不使用最速下降法"><a href="#为何不使用最速下降法" class="headerlink" title="为何不使用最速下降法"></a>为何不使用最速下降法</h2><p>梯度方向下降虽然幅度很大，但在迭代次数上却可能非常的多，下降过程很有可能是Z字形的。我们最理想的状态应该是：对于一个<script type="math/tex">n</script>维的空间，选择<script type="math/tex">n</script>个正交的向量，在每一个向量的方向上迭代一次，最终得出结果。换言之：当前的误差（记做<script type="math/tex">e_i = x_i - \bar{x}</script>，其中<script type="math/tex">\bar{x}</script>即为我们最终所求）和之前迭代的方向应该是正交的。所以<script type="math/tex">p_i^Te_{i + 1} = p_i^T(e_i + \alpha_ip_i) = 0</script>，于是我们得到：<script type="math/tex">\alpha_i = -\frac{p_i^Te_i}{p_i^Tp_i}</script>。</p>
<p>我们有了<script type="math/tex">\alpha_i</script>的式子，然而我们并不能得到<script type="math/tex">\alpha_i</script>：我们并不知道<script type="math/tex">e_i</script>的值（否则我们就知道答案了）。为此，我们需要使用矩阵向量正交（A-orthogonal）代替我们原本的正交。</p>
<p>如果两个向量<script type="math/tex">x, y</script>满足<script type="math/tex">x^TAy = 0</script>，我们称<script type="math/tex">x</script>A正交<script type="math/tex">y</script>，或<script type="math/tex">x</script>A<strong>共轭</strong><script type="math/tex">y</script>。当A为单位矩阵时，式子退化为普通的正交公式。</p>
<p>接下来我们继续研究如何找到<script type="math/tex">\alpha_i</script>。</p>
<h1 id="Gram-Schmidt正交化"><a href="#Gram-Schmidt正交化" class="headerlink" title="Gram-Schmidt正交化"></a>Gram-Schmidt正交化</h1><h2 id="普通的Gram-Schmidt正交化"><a href="#普通的Gram-Schmidt正交化" class="headerlink" title="普通的Gram-Schmidt正交化"></a>普通的Gram-Schmidt正交化</h2><p>对于普通的Gram-Schmidt正交化，我们首先选取两个向量<script type="math/tex">v_1, v_2</script>，并令<script type="math/tex">u_1 = v_1, u_2 = v_2 - k_u1</script>，此时的<script type="math/tex">u_1, u_2</script>应该是正交的。<br>对于以后的任意<script type="math/tex">u_i</script>，我们也都有：<script type="math/tex">u_i = v_i - \sum_{j = 1}^{i - 1}k_ju_j</script>。这样我们就完成了一组向量的正交化。</p>
<p>这个算法的本质是去除当前向量在其他正交向量上的投影，这样就能使当前向量与其他向量也保持正交。</p>
<h2 id="Gram-Schmidt正交化改"><a href="#Gram-Schmidt正交化改" class="headerlink" title="Gram-Schmidt正交化改"></a>Gram-Schmidt正交化改</h2><p>为了得到A共轭的一组向量，我们也需要使用Gram-Schmidt正交化，只不过在求系数<script type="math/tex">k</script>时，我们要做一些手脚。</p>
<p>因为</p>
<script type="math/tex; mode=display">u_i^TAu_j = v_i^TAu_j + \sum_{k = 1}^{i - 1}\beta_{ik}v_k^TAv_j = v_i^TAu_j + \beta_{ij}v_j^TAv_j = 0</script><p>所以我们可以求得：<script type="math/tex">\beta_{ij} = -\frac{v_i^TAu_j}{v_j^TAv_j}</script>。</p>
<p>如果我们要使用这个式子，我们需要面对复杂度过高的麻烦。所以在早期，这个算法都不大受人待见，直到共轭梯度被提出。</p>
<h1 id="共轭梯度法"><a href="#共轭梯度法" class="headerlink" title="共轭梯度法"></a>共轭梯度法</h1><p>我们将原来的式子带回到Gram-Schmidt正交化中：我们不妨改写<script type="math/tex">\beta_{i + 1, i}</script>为<script type="math/tex">\beta_{i}</script>（对于其他的情况来说，<script type="math/tex">\beta</script>为0，我们便不再做考虑），所以<script type="math/tex">\beta_i = -\frac{r_{i}^TAp_{i - 1} }{p_{i - 1}^TAp_{i - 1} }</script>。</p>
<p>因为<script type="math/tex">r_{j + 1} = r_j - \alpha_jAp_j</script>，我们在等式两侧同时左乘上<script type="math/tex">r_k^T</script>：<script type="math/tex">r_k^Tr_{j + 1} = r_k^Tr_j - \alpha_jr_k^TAp_j</script>，我们就可以拿到<script type="math/tex">r_k^TAp_j</script>的表达式。</p>
<p>此外，我们还可以得到一个有趣的性质：由于我们的方向选取是基于误差选取的，所以<script type="math/tex">p_i = r_i + \sum_{k = 0}^{i - 1}\beta_{ik}p_k</script>，所以<script type="math/tex">p_i^Tr_j = r_i^Tr_j</script>，当<script type="math/tex">i \ne j</script>时，两个向量正交，所以内积为0。即我们的误差之间也是正交的。</p>
<p>这下我们就可以进行化简了：</p>
<script type="math/tex; mode=display">\beta_i = -\frac{r_{i}^TAp_{i - 1} }{p_{i - 1}^TAp_{i - 1} } = \frac{r_i^Tr_i}{p_{i - 1}^Tr_{i - 1}}</script><p>最终我们的迭代公式为：</p>
<ul>
<li><script type="math/tex; mode=display">\alpha_i = \frac{r_i^Tr_i}{r_i^TAr_i}</script></li>
<li><script type="math/tex; mode=display">x_{i + 1} = x_i + \alpha_iAp_i</script></li>
<li><script type="math/tex; mode=display">r_{i + 1} = r_i - \alpha_iAp_i</script></li>
<li><script type="math/tex; mode=display">\beta_i = \frac{r_i^Tr_i}{p_{i - 1}^Tr_{i - 1}}</script></li>
<li><script type="math/tex; mode=display">p_{i + 1} = r_{i + 1} + \beta_ip_i</script></li>
</ul>
</div><div class="p-copyright"><blockquote><div class="p-copyright-author"><span class="p-copyright-key">Author：</span><span class="p-copytight-value"><a href="mailto:litreily@163.com">NeilKleistGao</a></span></div><div class="p-copyright-link"><span class="p-copyright-key">Link：</span><span class="p-copytight-value"><a href="/2020/10/08/conjugate-gradient/">http://example.com/2020/10/08/conjugate-gradient/</a></span></div><div class="p-copyright-note"><span class="p-copyright-key">Copyright：</span><span class="p-copytight-value">All articles in this blog use<a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/"> CC BY-NC 4.0 </a>license。Please indicate the source when reprinting  <a href="http://example.com">NeilKleistGao's blog</a>！</span></div></blockquote></div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tags"></i><a href="/tags/Linear-Algebra/">Linear Algebra</a><a href="/tags/Numerical-Analysis/">Numerical Analysis</a><a href="/tags/Calculus/">Calculus</a></span></div><aside id="toc"><div class="toc-title">CATALOG</div><nav><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E6%B1%82%E5%AF%BC"><span class="toc-number">2.</span> <span class="toc-text">向量求导</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">最速下降法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E8%BD%AC%E6%8D%A2"><span class="toc-number">3.1.</span> <span class="toc-text">问题转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BD%95%E4%B8%8D%E4%BD%BF%E7%94%A8%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">为何不使用最速下降法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Gram-Schmidt%E6%AD%A3%E4%BA%A4%E5%8C%96"><span class="toc-number">4.</span> <span class="toc-text">Gram-Schmidt正交化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%99%AE%E9%80%9A%E7%9A%84Gram-Schmidt%E6%AD%A3%E4%BA%A4%E5%8C%96"><span class="toc-number">4.1.</span> <span class="toc-text">普通的Gram-Schmidt正交化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gram-Schmidt%E6%AD%A3%E4%BA%A4%E5%8C%96%E6%94%B9"><span class="toc-number">4.2.</span> <span class="toc-text">Gram-Schmidt正交化改</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">共轭梯度法</span></a></li></ol></nav></aside></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="next" href="/2020/10/06/cpp-everything-can-work-0/">C++ Everything Can Work 0x0000： Order of Initialization &gt;</a></div></section><script>MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ["$","$"], ["\\(","\\)"] ],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
        processEscapes: true
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax();
    for (var i = 0; i < all.length; ++i)
        all[i].SourceElement().parentNode.className += ' has-jax';
});</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><footer><p>Copyright © 2016 - 2020 <a href="/." rel="nofollow">NeilKleistGao's Blog</a> | <strong><a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></strong><br>Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a>Theme with<a rel="nofollow" target="_blank" href="https://github.com/litreily/snark-hexo"> snark.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script></body></html>